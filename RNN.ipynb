{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOe8jojM5nxHCXtloQ1NtDV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fantasybarry/MSFT-Prediction/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attempt to forecast the price of MSFT by analyzing the prices of multiple stocks, including MSFT, over several consecutive days leading up to the target day."
      ],
      "metadata": {
        "id": "PelwS7CJcuW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup from HW2(Modified)\n"
      ],
      "metadata": {
        "id": "b1wXsETecwzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "class StockDataset(data.Dataset):\n",
        "    def __init__(self,X,Y,days):\n",
        "        self.X = X\n",
        "        self.Y = Y.reshape(-1)\n",
        "        self.days = days\n",
        "\n",
        "        # Store normalization parameters\n",
        "        self.X_mean = X.mean(axis=1, keepdims=True)\n",
        "        self.X_std = X.std(axis=1, keepdims=True)\n",
        "        self.Y_mean = Y.mean()\n",
        "        self.Y_std = Y.std()\n",
        "\n",
        "        # Normalize data\n",
        "        self.X = (self.X - self.X_mean) / self.X_std\n",
        "        self.Y = (self.Y - self.Y_mean) / self.Y_std\n",
        "\n",
        "    def inverse_transform(self, y):\n",
        "        return y * self.Y_std + self.Y_mean\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.Y)-self.days)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        x = self.X[:, index:index+self.days].T  # Shape: [seq_len, num_features]\n",
        "        y = self.Y[index + self.days]\n",
        "        return torch.tensor(x, dtype = torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "mCRue04Gc3Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import exp, sum, log, log10\n",
        "\n",
        "# Fixed data fetching\n",
        "def get_price(tickers, start='2020-01-01', end=None):\n",
        "    df = pd.DataFrame()\n",
        "    for ticker in tickers:\n",
        "        data = yf.Ticker(ticker).history(start=start, end=end)\n",
        "        df[ticker] = data['Close']\n",
        "    return df\n",
        "\n",
        "feature_stocks=['tsla','meta','nvda','amzn','nflx','gbtc','gdx','intc','dal','c','goog','aapl','msft','ibm','hp','orcl','sap','crm','hubs','twlo']\n",
        "predict_stock='msft'\n",
        "\n",
        "# getting data\n",
        "start_date='2020-01-01'\n",
        "\n",
        "allX=get_price(feature_stocks,start=start_date)\n",
        "ally=get_price([predict_stock],start=start_date)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = allX.to_numpy().transpose().astype(np.float32)\n",
        "y = ally.to_numpy().astype(np.float32)"
      ],
      "metadata": {
        "id": "He21dQ6rdARU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import torch\n",
        "\n",
        "# Initialize with different days values (try 5, 10, 20, 32)\n",
        "days_window = 32  # <-- Adjust this value (1-32)\n",
        "stockData = StockDataset(X, y, days=days_window)\n",
        "\n",
        "train_set_size = int(len(stockData)*0.7)\n",
        "valid_set_size = int(len(stockData)*0.2)\n",
        "test_set_size = len(stockData)-train_set_size-valid_set_size\n",
        "\n",
        "train_set, valid_set, test_set = data.random_split(stockData,[train_set_size,valid_set_size,test_set_size],\n",
        "                                              generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "batch_size = train_set_size # use entire dataset as batch\n",
        "train_dataloader = DataLoader(train_set,batch_size=batch_size,shuffle=True)  # input:(20,5), label:1\n",
        "valid_dataloader = DataLoader(valid_set,batch_size=batch_size,shuffle=False)\n",
        "test_dataloader = DataLoader(test_set,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "50nMSIj7dFIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: RNN with at least one LSTM layer."
      ],
      "metadata": {
        "id": "q6i0oa2OfWBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size = 64, num_layers = 1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, dropout=0.2)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x) # x shape: [batch, seq_len, features]\n",
        "        return self.linear(out[:, -1, :]).squeeze()\n",
        "\n",
        "    # Training setup\n",
        "batch_size = 32\n",
        "train_dataloader = data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = data.DataLoader(valid_set, batch_size=batch_size)\n",
        "test_dataloader = data.DataLoader(test_set, batch_size=batch_size)\n",
        "\n",
        "model = StockLSTM(input_size=len(feature_stocks), hidden_size=64, num_layers=1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = sum(criterion(model(X_val), y_val) for X_val, y_val in valid_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss/len(valid_dataloader):.4f}\")\n",
        "\n",
        "def forecast_future(model, dataset, last_known_index):\n",
        "    # Extract the last 'days'-length sequence\n",
        "    x = dataset.X[:, last_known_index-dataset.days : last_known_index].T\n",
        "    x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(x)\n",
        "\n",
        "    return dataset.inverse_transform(prediction.item())\n",
        "\n",
        "last_idx = len(stockData) - 1  # Most recent data point\n",
        "predicted_price = forecast_future(model, stockData, last_idx)\n",
        "print(f\"Next-day MSFT price prediction: ${predicted_price:.2f}\")\n",
        "\n",
        "# Add to validation/test phases\n",
        "def evaluate_model(dataloader, dataset):\n",
        "    model.eval()\n",
        "    predictions, truths = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            preds = model(X)\n",
        "            preds = dataset.inverse_transform(preds.numpy())\n",
        "            y = dataset.inverse_transform(y.numpy())\n",
        "            predictions.extend(preds)\n",
        "            truths.extend(y)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = np.mean(np.abs(np.array(predictions) - np.array(truths)))\n",
        "    rmse = np.sqrt(np.mean((np.array(predictions) - np.array(truths))**2))\n",
        "    return mae, rmse\n",
        "\n",
        "# Use during validation\n",
        "val_mae, val_rmse = evaluate_model(valid_dataloader, stockData)\n",
        "print(f\"Validation MAE: ${val_mae:.2f}, RMSE: ${val_rmse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMmqb4TNfK4V",
        "outputId": "ace274b1-5cee-40cb-8647-3b3bc415c5df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "<ipython-input-7-a2ae803d4573>:38: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  val_loss = sum(criterion(model(X_val), y_val) for X_val, y_val in valid_dataloader)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Loss: 0.3519\n",
            "Epoch 2, Val Loss: 0.0471\n",
            "Epoch 3, Val Loss: 0.0250\n",
            "Epoch 4, Val Loss: 0.0197\n",
            "Epoch 5, Val Loss: 0.0167\n",
            "Epoch 6, Val Loss: 0.0134\n",
            "Epoch 7, Val Loss: 0.0119\n",
            "Epoch 8, Val Loss: 0.0100\n",
            "Epoch 9, Val Loss: 0.0117\n",
            "Epoch 10, Val Loss: 0.0091\n",
            "Epoch 11, Val Loss: 0.0088\n",
            "Epoch 12, Val Loss: 0.0091\n",
            "Epoch 13, Val Loss: 0.0082\n",
            "Epoch 14, Val Loss: 0.0083\n",
            "Epoch 15, Val Loss: 0.0075\n",
            "Epoch 16, Val Loss: 0.0076\n",
            "Epoch 17, Val Loss: 0.0082\n",
            "Epoch 18, Val Loss: 0.0071\n",
            "Epoch 19, Val Loss: 0.0066\n",
            "Epoch 20, Val Loss: 0.0076\n",
            "Epoch 21, Val Loss: 0.0074\n",
            "Epoch 22, Val Loss: 0.0066\n",
            "Epoch 23, Val Loss: 0.0088\n",
            "Epoch 24, Val Loss: 0.0062\n",
            "Epoch 25, Val Loss: 0.0064\n",
            "Epoch 26, Val Loss: 0.0061\n",
            "Epoch 27, Val Loss: 0.0068\n",
            "Epoch 28, Val Loss: 0.0091\n",
            "Epoch 29, Val Loss: 0.0066\n",
            "Epoch 30, Val Loss: 0.0059\n",
            "Epoch 31, Val Loss: 0.0060\n",
            "Epoch 32, Val Loss: 0.0064\n",
            "Epoch 33, Val Loss: 0.0064\n",
            "Epoch 34, Val Loss: 0.0060\n",
            "Epoch 35, Val Loss: 0.0058\n",
            "Epoch 36, Val Loss: 0.0060\n",
            "Epoch 37, Val Loss: 0.0057\n",
            "Epoch 38, Val Loss: 0.0073\n",
            "Epoch 39, Val Loss: 0.0072\n",
            "Epoch 40, Val Loss: 0.0064\n",
            "Epoch 41, Val Loss: 0.0072\n",
            "Epoch 42, Val Loss: 0.0057\n",
            "Epoch 43, Val Loss: 0.0064\n",
            "Epoch 44, Val Loss: 0.0058\n",
            "Epoch 45, Val Loss: 0.0064\n",
            "Epoch 46, Val Loss: 0.0064\n",
            "Epoch 47, Val Loss: 0.0058\n",
            "Epoch 48, Val Loss: 0.0056\n",
            "Epoch 49, Val Loss: 0.0064\n",
            "Epoch 50, Val Loss: 0.0057\n",
            "Next-day MSFT price prediction: $383.29\n",
            "Validation MAE: $4.77, RMSE: $6.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Attention Network with at least one self-attention layer"
      ],
      "metadata": {
        "id": "46DjPvYWfbNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention-based model with same data pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class StockAttentionModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_heads=2):\n",
        "        super().__init__()\n",
        "        # Position-wise feedforward\n",
        "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Self-attention layer\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Temporal processing\n",
        "        self.temporal = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Final prediction\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, num_features]\n",
        "        x = F.relu(self.input_proj(x))\n",
        "\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.self_attn(x, x, x)\n",
        "\n",
        "        # Temporal processing\n",
        "        temp_out, _ = self.temporal(attn_out)\n",
        "\n",
        "        # Use last timestep for prediction\n",
        "        return self.regressor(temp_out[:, -1, :]).squeeze()\n",
        "\n",
        "# Modified training setup\n",
        "model = StockAttentionModel(\n",
        "    input_size=len(feature_stocks),\n",
        "    hidden_size=128,\n",
        "    num_heads=4\n",
        ")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "# Training loop with attention-specific adjustments\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        for X_val, y_val in valid_dataloader:\n",
        "            outputs = model(X_val)\n",
        "            val_loss += criterion(outputs, y_val).item()\n",
        "        val_loss /= len(valid_dataloader)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    print(f\"Epoch {epoch+1}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Evaluation with attention model\n",
        "def attention_forecast(model, dataset, last_index):\n",
        "    # Ensure valid index range\n",
        "    if last_index < dataset.days:\n",
        "        raise ValueError(f\"Index must be >= {dataset.days}\")\n",
        "\n",
        "    # Get input sequence\n",
        "    x = dataset.X[:, last_index-dataset.days : last_index].T\n",
        "    x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Make prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred = model(x)\n",
        "\n",
        "    # Convert and return scalar value\n",
        "    return dataset.inverse_transform(pred.item())  # No index needed for scalar\n",
        "\n",
        "\n",
        "# Enhanced evaluation with attention weights visualization\n",
        "def plot_attention(model, sample_input):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _, attn_weights = model.self_attn(\n",
        "            sample_input, sample_input, sample_input\n",
        "        )\n",
        "    plt.matshow(attn_weights[0].numpy())\n",
        "    plt.title('Attention Pattern for Last Prediction')\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(dataloader, dataset):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    truths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # Get predictions\n",
        "            preds = model(X)\n",
        "\n",
        "            # Convert to numpy arrays and denormalize\n",
        "            preds_denorm = dataset.inverse_transform(preds.numpy().flatten())\n",
        "            y_denorm = dataset.inverse_transform(y.numpy().flatten())\n",
        "\n",
        "            predictions.extend(preds_denorm)\n",
        "            truths.extend(y_denorm)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = np.mean(np.abs(np.array(predictions) - np.array(truths)))\n",
        "    rmse = np.sqrt(np.mean((np.array(predictions) - np.array(truths))**2))\n",
        "\n",
        "    return mae, rmse\n",
        "\n",
        "# Usage with validation set\n",
        "val_mae, val_rmse = evaluate_model(valid_dataloader, stockData)\n",
        "print(f\"\\nValidation Metrics:\")\n",
        "print(f\"MAE: ${val_mae:.2f}\")\n",
        "print(f\"RMSE: ${val_rmse:.2f}\")\n",
        "\n",
        "# For test set\n",
        "test_mae, test_rmse = evaluate_model(test_dataloader, stockData)\n",
        "print(f\"\\nTest Metrics:\")\n",
        "print(f\"MAE: ${test_mae:.2f}\")\n",
        "print(f\"RMSE: ${test_rmse:.2f}\")\n",
        "\n",
        "# Example usage\n",
        "last_idx = len(stockData) - 1\n",
        "pred_price = attention_forecast(model, stockData, last_idx)\n",
        "print(f\"Attention Model Prediction of MSFT stock price: ${pred_price:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCmvEyS0fctB",
        "outputId": "a334e765-01a8-4fd3-9451-f32fadb0c3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Val Loss: 0.6949\n",
            "Epoch 2, Val Loss: 0.2180\n",
            "Epoch 3, Val Loss: 0.0725\n",
            "Epoch 4, Val Loss: 0.0433\n",
            "Epoch 5, Val Loss: 0.0365\n",
            "Epoch 6, Val Loss: 0.0320\n",
            "Epoch 7, Val Loss: 0.0285\n",
            "Epoch 8, Val Loss: 0.0274\n",
            "Epoch 9, Val Loss: 0.0252\n",
            "Epoch 10, Val Loss: 0.0238\n",
            "Epoch 11, Val Loss: 0.0231\n",
            "Epoch 12, Val Loss: 0.0232\n",
            "Epoch 13, Val Loss: 0.0229\n",
            "Epoch 14, Val Loss: 0.0242\n",
            "Epoch 15, Val Loss: 0.0199\n",
            "Epoch 16, Val Loss: 0.0183\n",
            "Epoch 17, Val Loss: 0.0175\n",
            "Epoch 18, Val Loss: 0.0172\n",
            "Epoch 19, Val Loss: 0.0166\n",
            "Epoch 20, Val Loss: 0.0161\n",
            "Epoch 21, Val Loss: 0.0164\n",
            "Epoch 22, Val Loss: 0.0160\n",
            "Epoch 23, Val Loss: 0.0156\n",
            "Epoch 24, Val Loss: 0.0160\n",
            "Epoch 25, Val Loss: 0.0160\n",
            "Epoch 26, Val Loss: 0.0158\n",
            "Epoch 27, Val Loss: 0.0143\n",
            "Epoch 28, Val Loss: 0.0147\n",
            "Epoch 29, Val Loss: 0.0144\n",
            "Epoch 30, Val Loss: 0.0132\n",
            "Epoch 31, Val Loss: 0.0129\n",
            "Epoch 32, Val Loss: 0.0133\n",
            "Epoch 33, Val Loss: 0.0152\n",
            "Epoch 34, Val Loss: 0.0139\n",
            "Epoch 35, Val Loss: 0.0141\n",
            "Epoch 36, Val Loss: 0.0133\n",
            "Epoch 37, Val Loss: 0.0126\n",
            "Epoch 38, Val Loss: 0.0124\n",
            "Epoch 39, Val Loss: 0.0122\n",
            "Epoch 40, Val Loss: 0.0122\n",
            "Epoch 41, Val Loss: 0.0119\n",
            "Epoch 42, Val Loss: 0.0121\n",
            "Epoch 43, Val Loss: 0.0120\n",
            "Epoch 44, Val Loss: 0.0119\n",
            "Epoch 45, Val Loss: 0.0120\n",
            "Epoch 46, Val Loss: 0.0120\n",
            "Epoch 47, Val Loss: 0.0119\n",
            "Epoch 48, Val Loss: 0.0118\n",
            "Epoch 49, Val Loss: 0.0118\n",
            "Epoch 50, Val Loss: 0.0119\n",
            "Epoch 51, Val Loss: 0.0118\n",
            "Epoch 52, Val Loss: 0.0119\n",
            "Epoch 53, Val Loss: 0.0118\n",
            "Epoch 54, Val Loss: 0.0118\n",
            "Epoch 55, Val Loss: 0.0118\n",
            "Epoch 56, Val Loss: 0.0118\n",
            "Epoch 57, Val Loss: 0.0118\n",
            "Epoch 58, Val Loss: 0.0118\n",
            "Epoch 59, Val Loss: 0.0118\n",
            "Epoch 60, Val Loss: 0.0118\n",
            "Epoch 61, Val Loss: 0.0118\n",
            "Epoch 62, Val Loss: 0.0118\n",
            "Epoch 63, Val Loss: 0.0118\n",
            "Epoch 64, Val Loss: 0.0118\n",
            "Epoch 65, Val Loss: 0.0118\n",
            "Epoch 66, Val Loss: 0.0118\n",
            "Epoch 67, Val Loss: 0.0118\n",
            "Epoch 68, Val Loss: 0.0118\n",
            "Epoch 69, Val Loss: 0.0118\n",
            "Epoch 70, Val Loss: 0.0118\n",
            "Epoch 71, Val Loss: 0.0118\n",
            "Epoch 72, Val Loss: 0.0118\n",
            "Epoch 73, Val Loss: 0.0118\n",
            "Epoch 74, Val Loss: 0.0118\n",
            "Epoch 75, Val Loss: 0.0118\n",
            "Epoch 76, Val Loss: 0.0118\n",
            "Epoch 77, Val Loss: 0.0118\n",
            "Epoch 78, Val Loss: 0.0118\n",
            "Epoch 79, Val Loss: 0.0118\n",
            "Epoch 80, Val Loss: 0.0118\n",
            "Epoch 81, Val Loss: 0.0118\n",
            "Epoch 82, Val Loss: 0.0118\n",
            "Epoch 83, Val Loss: 0.0118\n",
            "Epoch 84, Val Loss: 0.0118\n",
            "Epoch 85, Val Loss: 0.0118\n",
            "Epoch 86, Val Loss: 0.0118\n",
            "Epoch 87, Val Loss: 0.0118\n",
            "Epoch 88, Val Loss: 0.0118\n",
            "Epoch 89, Val Loss: 0.0118\n",
            "Epoch 90, Val Loss: 0.0118\n",
            "Epoch 91, Val Loss: 0.0118\n",
            "Epoch 92, Val Loss: 0.0118\n",
            "Epoch 93, Val Loss: 0.0118\n",
            "Epoch 94, Val Loss: 0.0118\n",
            "Epoch 95, Val Loss: 0.0118\n",
            "Epoch 96, Val Loss: 0.0118\n",
            "Epoch 97, Val Loss: 0.0118\n",
            "Epoch 98, Val Loss: 0.0118\n",
            "Epoch 99, Val Loss: 0.0118\n",
            "Epoch 100, Val Loss: 0.0118\n",
            "\n",
            "Validation Metrics:\n",
            "MAE: $7.37\n",
            "RMSE: $9.21\n",
            "\n",
            "Test Metrics:\n",
            "MAE: $7.06\n",
            "RMSE: $8.66\n",
            "Attention Model Prediction of MSFT stock price: $386.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Knowledge distillation to shrink the attention network model by half\n"
      ],
      "metadata": {
        "id": "DDl10p4mghzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Set up, Size reduction"
      ],
      "metadata": {
        "id": "UbrBOMGrgj1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Teacher and Student Models\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(input_size, 4, batch_first=True)\n",
        "        self.lstm = nn.LSTM(input_size, 128, num_layers=2, batch_first=True)\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        lstm_out, _ = self.lstm(attn_out)\n",
        "        return self.regressor(lstm_out[:, -1, :]).squeeze()\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        # 50% smaller model\n",
        "        self.attn = nn.MultiheadAttention(input_size, 2, batch_first=True)\n",
        "        self.lstm = nn.LSTM(input_size, 64, num_layers=1, batch_first=True)\n",
        "        self.regressor = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        lstm_out, _ = self.lstm(attn_out)\n",
        "        return self.regressor(lstm_out[:, -1, :]).squeeze()\n",
        "\n",
        "# 2. Initialize models and trainers\n",
        "teacher = TeacherModel(len(feature_stocks))\n",
        "student = StudentModel(len(feature_stocks))\n",
        "\n",
        "# Fixed evaluate_model function\n",
        "def evaluate_model(dataloader, model, dataset):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    truths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            preds = model(X)\n",
        "            preds_denorm = dataset.inverse_transform(preds.numpy().flatten())\n",
        "            y_denorm = dataset.inverse_transform(y.numpy().flatten())\n",
        "            predictions.extend(preds_denorm)\n",
        "            truths.extend(y_denorm)\n",
        "\n",
        "    mae = np.mean(np.abs(np.array(predictions) - np.array(truths)))\n",
        "    rmse = np.sqrt(np.mean((np.array(predictions) - np.array(truths))**2))\n",
        "    return mae, rmse\n",
        "\n",
        "# Fixed training loop with correct function calls\n",
        "def train_jointly(teacher, student, epochs=100):\n",
        "    # Train teacher first\n",
        "    teacher_optim = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"Training Teacher Model:\")\n",
        "    for epoch in range(epochs//2):\n",
        "        teacher.train()\n",
        "        for X, y in train_dataloader:\n",
        "            teacher_optim.zero_grad()\n",
        "            preds = teacher(X)\n",
        "            loss = F.mse_loss(preds, y)\n",
        "            loss.backward()\n",
        "            teacher_optim.step()\n",
        "\n",
        "        # Validation\n",
        "        teacher.eval()\n",
        "        val_mae, val_rmse = evaluate_model(valid_dataloader, teacher, stockData)\n",
        "        print(f\"Teacher Epoch {epoch+1} | Val MAE: ${val_mae:.2f} | Val RMSE: ${val_rmse:.2f}\")\n",
        "\n",
        "    # Train student with teacher guidance\n",
        "    student_optim = torch.optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "    print(\"\\nTraining Student with Teacher Guidance:\")\n",
        "    for epoch in range(epochs//2):\n",
        "        student.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X, y in train_dataloader:\n",
        "            student_optim.zero_grad()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_preds = teacher(X)\n",
        "\n",
        "            student_preds = student(X)\n",
        "            loss = 0.3*F.mse_loss(student_preds, y) + 0.7*F.mse_loss(student_preds, teacher_preds)\n",
        "\n",
        "            loss.backward()\n",
        "            student_optim.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        student.eval()\n",
        "        val_mae, val_rmse = evaluate_model(valid_dataloader, student, stockData)\n",
        "        print(f\"Student Epoch {epoch+1} | Loss: {total_loss/len(train_dataloader):.4f} | Val MAE: ${val_mae:.2f} | Val RMSE: ${val_rmse:.2f}\")\n",
        "\n",
        "# 4. Run the joint training\n",
        "train_jointly(teacher, student, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JG6ldsugjBc",
        "outputId": "402d2343-19bf-453a-8174-4865cd83943d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Teacher Model:\n",
            "Teacher Epoch 1 | Val MAE: $22.33 | Val RMSE: $28.15\n",
            "Teacher Epoch 2 | Val MAE: $13.66 | Val RMSE: $17.33\n",
            "Teacher Epoch 3 | Val MAE: $11.10 | Val RMSE: $14.13\n",
            "Teacher Epoch 4 | Val MAE: $11.36 | Val RMSE: $14.17\n",
            "Teacher Epoch 5 | Val MAE: $10.57 | Val RMSE: $13.61\n",
            "Teacher Epoch 6 | Val MAE: $9.70 | Val RMSE: $12.35\n",
            "Teacher Epoch 7 | Val MAE: $9.92 | Val RMSE: $12.60\n",
            "Teacher Epoch 8 | Val MAE: $9.22 | Val RMSE: $11.70\n",
            "Teacher Epoch 9 | Val MAE: $9.54 | Val RMSE: $12.04\n",
            "Teacher Epoch 10 | Val MAE: $9.62 | Val RMSE: $11.98\n",
            "Teacher Epoch 11 | Val MAE: $11.04 | Val RMSE: $13.67\n",
            "Teacher Epoch 12 | Val MAE: $8.85 | Val RMSE: $11.50\n",
            "Teacher Epoch 13 | Val MAE: $9.23 | Val RMSE: $11.65\n",
            "Teacher Epoch 14 | Val MAE: $10.08 | Val RMSE: $12.19\n",
            "Teacher Epoch 15 | Val MAE: $9.71 | Val RMSE: $12.18\n",
            "Teacher Epoch 16 | Val MAE: $8.19 | Val RMSE: $10.54\n",
            "Teacher Epoch 17 | Val MAE: $8.43 | Val RMSE: $10.71\n",
            "Teacher Epoch 18 | Val MAE: $9.09 | Val RMSE: $11.40\n",
            "Teacher Epoch 19 | Val MAE: $8.18 | Val RMSE: $10.39\n",
            "Teacher Epoch 20 | Val MAE: $8.90 | Val RMSE: $10.62\n",
            "Teacher Epoch 21 | Val MAE: $8.13 | Val RMSE: $10.26\n",
            "Teacher Epoch 22 | Val MAE: $8.91 | Val RMSE: $11.43\n",
            "Teacher Epoch 23 | Val MAE: $7.93 | Val RMSE: $9.93\n",
            "Teacher Epoch 24 | Val MAE: $8.13 | Val RMSE: $10.31\n",
            "Teacher Epoch 25 | Val MAE: $8.29 | Val RMSE: $10.16\n",
            "Teacher Epoch 26 | Val MAE: $8.91 | Val RMSE: $11.40\n",
            "Teacher Epoch 27 | Val MAE: $7.69 | Val RMSE: $9.80\n",
            "Teacher Epoch 28 | Val MAE: $7.71 | Val RMSE: $9.68\n",
            "Teacher Epoch 29 | Val MAE: $7.93 | Val RMSE: $9.96\n",
            "Teacher Epoch 30 | Val MAE: $8.27 | Val RMSE: $10.51\n",
            "Teacher Epoch 31 | Val MAE: $6.77 | Val RMSE: $8.43\n",
            "Teacher Epoch 32 | Val MAE: $7.70 | Val RMSE: $9.42\n",
            "Teacher Epoch 33 | Val MAE: $7.32 | Val RMSE: $9.15\n",
            "Teacher Epoch 34 | Val MAE: $7.15 | Val RMSE: $8.94\n",
            "Teacher Epoch 35 | Val MAE: $7.86 | Val RMSE: $9.87\n",
            "Teacher Epoch 36 | Val MAE: $7.08 | Val RMSE: $8.96\n",
            "Teacher Epoch 37 | Val MAE: $7.49 | Val RMSE: $9.36\n",
            "Teacher Epoch 38 | Val MAE: $7.58 | Val RMSE: $9.51\n",
            "Teacher Epoch 39 | Val MAE: $7.20 | Val RMSE: $9.13\n",
            "Teacher Epoch 40 | Val MAE: $6.94 | Val RMSE: $8.57\n",
            "Teacher Epoch 41 | Val MAE: $6.49 | Val RMSE: $8.32\n",
            "Teacher Epoch 42 | Val MAE: $6.80 | Val RMSE: $8.45\n",
            "Teacher Epoch 43 | Val MAE: $6.57 | Val RMSE: $8.12\n",
            "Teacher Epoch 44 | Val MAE: $6.64 | Val RMSE: $8.20\n",
            "Teacher Epoch 45 | Val MAE: $6.39 | Val RMSE: $8.12\n",
            "Teacher Epoch 46 | Val MAE: $6.34 | Val RMSE: $8.05\n",
            "Teacher Epoch 47 | Val MAE: $6.65 | Val RMSE: $8.29\n",
            "Teacher Epoch 48 | Val MAE: $6.89 | Val RMSE: $8.99\n",
            "Teacher Epoch 49 | Val MAE: $6.30 | Val RMSE: $7.86\n",
            "Teacher Epoch 50 | Val MAE: $6.50 | Val RMSE: $8.06\n",
            "\n",
            "Training Student with Teacher Guidance:\n",
            "Student Epoch 1 | Loss: 0.3510 | Val MAE: $16.69 | Val RMSE: $20.91\n",
            "Student Epoch 2 | Loss: 0.0556 | Val MAE: $13.10 | Val RMSE: $16.81\n",
            "Student Epoch 3 | Loss: 0.0331 | Val MAE: $11.40 | Val RMSE: $14.48\n",
            "Student Epoch 4 | Loss: 0.0239 | Val MAE: $10.88 | Val RMSE: $13.46\n",
            "Student Epoch 5 | Loss: 0.0185 | Val MAE: $9.81 | Val RMSE: $12.31\n",
            "Student Epoch 6 | Loss: 0.0145 | Val MAE: $9.40 | Val RMSE: $11.82\n",
            "Student Epoch 7 | Loss: 0.0117 | Val MAE: $8.93 | Val RMSE: $11.57\n",
            "Student Epoch 8 | Loss: 0.0109 | Val MAE: $9.04 | Val RMSE: $11.46\n",
            "Student Epoch 9 | Loss: 0.0099 | Val MAE: $9.03 | Val RMSE: $11.28\n",
            "Student Epoch 10 | Loss: 0.0104 | Val MAE: $9.15 | Val RMSE: $11.27\n",
            "Student Epoch 11 | Loss: 0.0100 | Val MAE: $9.04 | Val RMSE: $11.16\n",
            "Student Epoch 12 | Loss: 0.0095 | Val MAE: $8.67 | Val RMSE: $10.86\n",
            "Student Epoch 13 | Loss: 0.0087 | Val MAE: $9.45 | Val RMSE: $11.82\n",
            "Student Epoch 14 | Loss: 0.0086 | Val MAE: $8.92 | Val RMSE: $11.19\n",
            "Student Epoch 15 | Loss: 0.0087 | Val MAE: $9.05 | Val RMSE: $11.36\n",
            "Student Epoch 16 | Loss: 0.0084 | Val MAE: $8.47 | Val RMSE: $10.62\n",
            "Student Epoch 17 | Loss: 0.0082 | Val MAE: $8.61 | Val RMSE: $10.79\n",
            "Student Epoch 18 | Loss: 0.0082 | Val MAE: $8.53 | Val RMSE: $10.66\n",
            "Student Epoch 19 | Loss: 0.0076 | Val MAE: $8.47 | Val RMSE: $10.52\n",
            "Student Epoch 20 | Loss: 0.0073 | Val MAE: $8.35 | Val RMSE: $10.32\n",
            "Student Epoch 21 | Loss: 0.0068 | Val MAE: $8.33 | Val RMSE: $10.29\n",
            "Student Epoch 22 | Loss: 0.0070 | Val MAE: $8.13 | Val RMSE: $10.15\n",
            "Student Epoch 23 | Loss: 0.0066 | Val MAE: $8.31 | Val RMSE: $10.17\n",
            "Student Epoch 24 | Loss: 0.0068 | Val MAE: $8.40 | Val RMSE: $10.45\n",
            "Student Epoch 25 | Loss: 0.0068 | Val MAE: $8.21 | Val RMSE: $10.11\n",
            "Student Epoch 26 | Loss: 0.0062 | Val MAE: $7.83 | Val RMSE: $9.67\n",
            "Student Epoch 27 | Loss: 0.0061 | Val MAE: $7.79 | Val RMSE: $9.64\n",
            "Student Epoch 28 | Loss: 0.0055 | Val MAE: $7.52 | Val RMSE: $9.23\n",
            "Student Epoch 29 | Loss: 0.0052 | Val MAE: $7.53 | Val RMSE: $9.43\n",
            "Student Epoch 30 | Loss: 0.0050 | Val MAE: $7.31 | Val RMSE: $9.02\n",
            "Student Epoch 31 | Loss: 0.0054 | Val MAE: $7.31 | Val RMSE: $8.93\n",
            "Student Epoch 32 | Loss: 0.0046 | Val MAE: $7.68 | Val RMSE: $9.45\n",
            "Student Epoch 33 | Loss: 0.0047 | Val MAE: $7.20 | Val RMSE: $8.90\n",
            "Student Epoch 34 | Loss: 0.0057 | Val MAE: $7.67 | Val RMSE: $9.64\n",
            "Student Epoch 35 | Loss: 0.0049 | Val MAE: $7.40 | Val RMSE: $9.14\n",
            "Student Epoch 36 | Loss: 0.0045 | Val MAE: $7.18 | Val RMSE: $8.88\n",
            "Student Epoch 37 | Loss: 0.0043 | Val MAE: $7.29 | Val RMSE: $9.08\n",
            "Student Epoch 38 | Loss: 0.0042 | Val MAE: $7.19 | Val RMSE: $8.90\n",
            "Student Epoch 39 | Loss: 0.0042 | Val MAE: $7.28 | Val RMSE: $9.10\n",
            "Student Epoch 40 | Loss: 0.0041 | Val MAE: $7.57 | Val RMSE: $9.60\n",
            "Student Epoch 41 | Loss: 0.0039 | Val MAE: $7.16 | Val RMSE: $8.79\n",
            "Student Epoch 42 | Loss: 0.0039 | Val MAE: $7.05 | Val RMSE: $8.76\n",
            "Student Epoch 43 | Loss: 0.0040 | Val MAE: $7.01 | Val RMSE: $8.77\n",
            "Student Epoch 44 | Loss: 0.0038 | Val MAE: $7.13 | Val RMSE: $9.01\n",
            "Student Epoch 45 | Loss: 0.0038 | Val MAE: $7.14 | Val RMSE: $8.90\n",
            "Student Epoch 46 | Loss: 0.0035 | Val MAE: $6.94 | Val RMSE: $8.77\n",
            "Student Epoch 47 | Loss: 0.0036 | Val MAE: $6.74 | Val RMSE: $8.49\n",
            "Student Epoch 48 | Loss: 0.0033 | Val MAE: $6.74 | Val RMSE: $8.48\n",
            "Student Epoch 49 | Loss: 0.0034 | Val MAE: $6.78 | Val RMSE: $8.59\n",
            "Student Epoch 50 | Loss: 0.0038 | Val MAE: $6.75 | Val RMSE: $8.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Student Performance"
      ],
      "metadata": {
        "id": "1boGihnXhijj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Final Evaluation\n",
        "print(\"\\nFinal Student Performance:\")\n",
        "test_mae, test_rmse = evaluate_model(test_dataloader, student, stockData)\n",
        "print(f\"Test MAE: ${test_mae:.2f}\")\n",
        "print(f\"Test RMSE: ${test_rmse:.2f}\")\n",
        "\n",
        "# 6. Size Comparison\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"\\nModel Size Reduction: {count_params(student)/count_params(teacher):.1%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atCJyOOVhj4g",
        "outputId": "06e3aede-5da7-4ee1-c352-e035b942e6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Student Performance:\n",
            "Test MAE: $6.97\n",
            "Test RMSE: $8.59\n",
            "\n",
            "Model Size Reduction: 10.9%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-a8068ccf30ed>:9: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  return sum(p.numel() for p in model.parameters())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSFT stock price"
      ],
      "metadata": {
        "id": "hCkS4En-hm0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_msft_price(model, dataset, days=5):\n",
        "    \"\"\"Forecast next-day MSFT price using latest available data\"\"\"\n",
        "    # Get the last valid sequence window\n",
        "    last_idx = len(dataset) - 1\n",
        "\n",
        "    # Extract input sequence\n",
        "    x = dataset.X[:, last_idx-days:last_idx].T  # Shape: [seq_len, num_features]\n",
        "    x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Make prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred_normalized = model(x)\n",
        "\n",
        "    # Convert to actual price\n",
        "    pred_price = dataset.inverse_transform(pred_normalized.item())\n",
        "\n",
        "    # Get last actual price for reference\n",
        "    last_actual_price = dataset.inverse_transform(dataset.Y[last_idx])\n",
        "\n",
        "    return pred_price, last_actual_price\n",
        "\n",
        "# Example usage\n",
        "predicted_price, last_price = forecast_msft_price(student, stockData, days=5)\n",
        "\n",
        "print(\"\\nLatest MSFT Closing Price:\", f\"${last_price:.2f}\")\n",
        "print(\"Predicted Next-Day Price:\", f\"${predicted_price:.2f}\")\n",
        "print(\"Predicted Change:\", f\"{(predicted_price - last_price):.2f} ({((predicted_price/last_price)-1)*100:.2f}%)\")\n",
        "\n",
        "# Add technical validation\n",
        "val_mae, val_rmse = evaluate_model(valid_dataloader, student, stockData)\n",
        "print(\"\\nModel Validation Metrics:\")\n",
        "print(f\"Typical Error Range: ±${val_mae:.2f} (MAE)\")\n",
        "print(f\"Maximum Likely Error: ${val_rmse:.2f} (RMSE)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrlP1nN1hnNC",
        "outputId": "d923a8ce-e35c-4505-d1f0-b91b74825013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Latest MSFT Closing Price: $378.77\n",
            "Predicted Next-Day Price: $386.06\n",
            "Predicted Change: 7.29 (1.93%)\n",
            "\n",
            "Model Validation Metrics:\n",
            "Typical Error Range: ±$6.75 (MAE)\n",
            "Maximum Likely Error: $8.48 (RMSE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: Mamba Newtork"
      ],
      "metadata": {
        "id": "szoRO2LUhple"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model set up"
      ],
      "metadata": {
        "id": "MVh5kAy3hroy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mamba_ssm causal-conv1d\n",
        "!pip install mistral-inference"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CEJsKyMh3Ps",
        "outputId": "ad97b2bd-f64b-4ca1-f0fb-051df0650b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mamba_ssm in /usr/local/lib/python3.11/dist-packages (2.2.4)\n",
            "Requirement already satisfied: causal-conv1d in /usr/local/lib/python3.11/dist-packages (1.5.0.post8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (2.7.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (3.3.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (1.11.1.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (4.51.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.11/dist-packages (from mamba_ssm) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch->mamba_ssm) (1.11.1.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->mamba_ssm) (4.67.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch->mamba_ssm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->mamba_ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->mamba_ssm) (2025.1.31)\n",
            "Requirement already satisfied: mistral-inference in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: fire>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from mistral-inference) (0.7.0)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral-inference) (1.5.4)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral-inference) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistral-inference) (0.5.3)\n",
            "Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from mistral-inference) (0.1.7)\n",
            "Requirement already satisfied: xformers>=0.0.24 in /usr/local/lib/python3.11/dist-packages (from mistral-inference) (0.0.30)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.6.0->mistral-inference) (3.0.1)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (2.0.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (0.2.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral-inference) (4.13.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing>=0.1.5->mistral-inference) (0.16)\n",
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.11/dist-packages (from xformers>=0.0.24->mistral-inference) (2.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.7.0->xformers>=0.0.24->mistral-inference) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch==2.7.0->xformers>=0.0.24->mistral-inference) (75.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral-inference) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral-inference) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral-inference) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral-inference) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common>=1.5.4->mistral-inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common>=1.5.4->mistral-inference) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=2.7->mistral_common>=1.5.4->mistral-inference) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral-inference) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral-inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral-inference) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->mistral_common>=1.5.4->mistral-inference) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->mistral_common>=1.5.4->mistral-inference) (2024.11.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.7.0->xformers>=0.0.24->mistral-inference) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.7.0->xformers>=0.0.24->mistral-inference) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from mamba_ssm import Mamba\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MambaStockModel class\n",
        "class MambaStockModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, state_size=16):\n",
        "        super().__init__()\n",
        "        # Input projection\n",
        "        self.proj = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Mamba layer (state-space model)\n",
        "        self.mamba = Mamba(\n",
        "            d_model=hidden_size,  # Input dimension\n",
        "            d_state=state_size,   # State expansion factor\n",
        "            d_conv=4,             # Convolution kernel size\n",
        "            expand=2              # Expansion factor\n",
        "        ).to(device)\n",
        "\n",
        "        # Prediction head\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, num_features]\n",
        "        x = self.proj(x)          # Project to hidden dimension\n",
        "        x = self.mamba(x)         # Process with Mamba\n",
        "        return self.regressor(x[:, -1, :]).squeeze()  # Last timestep prediction\n",
        "\n",
        "# Initialize Mamba model\n",
        "mamba_model = MambaStockModel(\n",
        "    input_size=len(feature_stocks),\n",
        "    hidden_size=128,\n",
        "    state_size=32\n",
        ").to(device)\n",
        "\n",
        "# Modified training setup\n",
        "optimizer = torch.optim.AdamW(mamba_model.parameters(), lr=0.0002)\n",
        "criterion = nn.HuberLoss()  # More robust than MSE\n",
        "\n",
        "# Training loop with Mamba-specific adjustments\n",
        "for epoch in range(100):\n",
        "    mamba_model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Add sequence dimension (batch_size, seq_len, features)\n",
        "        outputs = mamba_model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(mamba_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    mamba_model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        predictions = []\n",
        "        truths = []\n",
        "\n",
        "        for X_val, y_val in valid_dataloader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            preds = mamba_model(X_val)\n",
        "            val_loss += criterion(preds, y_val).item()\n",
        "\n",
        "            # Denormalize\n",
        "            preds_denorm = stockData.inverse_transform(preds.cpu().numpy())\n",
        "            y_denorm = stockData.inverse_transform(y_val.cpu().numpy())\n",
        "            predictions.extend(preds_denorm)\n",
        "            truths.extend(y_denorm)\n",
        "\n",
        "        mae = np.mean(np.abs(np.array(predictions) - np.array(truths)))\n",
        "        rmse = np.sqrt(np.mean((np.array(predictions) - np.array(truths))**2))\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {epoch_loss/len(train_dataloader):.4f} | Val MAE: ${mae:.2f} | Val RMSE: ${rmse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14yMqR4khqgL",
        "outputId": "db252314-6ff5-4fbc-8ac8-70fe8561b99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.3861 | Val MAE: $52.10 | Val RMSE: $63.95\n",
            "Epoch 2 | Train Loss: 0.1930 | Val MAE: $28.62 | Val RMSE: $37.23\n",
            "Epoch 3 | Train Loss: 0.0512 | Val MAE: $13.65 | Val RMSE: $16.99\n",
            "Epoch 4 | Train Loss: 0.0152 | Val MAE: $10.05 | Val RMSE: $12.55\n",
            "Epoch 5 | Train Loss: 0.0084 | Val MAE: $7.97 | Val RMSE: $9.66\n",
            "Epoch 6 | Train Loss: 0.0059 | Val MAE: $6.84 | Val RMSE: $8.32\n",
            "Epoch 7 | Train Loss: 0.0048 | Val MAE: $6.39 | Val RMSE: $7.83\n",
            "Epoch 8 | Train Loss: 0.0046 | Val MAE: $6.23 | Val RMSE: $7.57\n",
            "Epoch 9 | Train Loss: 0.0040 | Val MAE: $5.91 | Val RMSE: $7.13\n",
            "Epoch 10 | Train Loss: 0.0037 | Val MAE: $5.66 | Val RMSE: $6.94\n",
            "Epoch 11 | Train Loss: 0.0034 | Val MAE: $5.51 | Val RMSE: $6.73\n",
            "Epoch 12 | Train Loss: 0.0032 | Val MAE: $5.45 | Val RMSE: $6.65\n",
            "Epoch 13 | Train Loss: 0.0031 | Val MAE: $5.31 | Val RMSE: $6.59\n",
            "Epoch 14 | Train Loss: 0.0030 | Val MAE: $5.48 | Val RMSE: $6.67\n",
            "Epoch 15 | Train Loss: 0.0029 | Val MAE: $5.54 | Val RMSE: $6.70\n",
            "Epoch 16 | Train Loss: 0.0028 | Val MAE: $5.22 | Val RMSE: $6.44\n",
            "Epoch 17 | Train Loss: 0.0028 | Val MAE: $5.76 | Val RMSE: $7.07\n",
            "Epoch 18 | Train Loss: 0.0026 | Val MAE: $5.07 | Val RMSE: $6.29\n",
            "Epoch 19 | Train Loss: 0.0026 | Val MAE: $5.17 | Val RMSE: $6.37\n",
            "Epoch 20 | Train Loss: 0.0027 | Val MAE: $4.99 | Val RMSE: $6.23\n",
            "Epoch 21 | Train Loss: 0.0026 | Val MAE: $5.01 | Val RMSE: $6.43\n",
            "Epoch 22 | Train Loss: 0.0026 | Val MAE: $5.00 | Val RMSE: $6.33\n",
            "Epoch 23 | Train Loss: 0.0025 | Val MAE: $4.90 | Val RMSE: $6.26\n",
            "Epoch 24 | Train Loss: 0.0025 | Val MAE: $5.26 | Val RMSE: $6.52\n",
            "Epoch 25 | Train Loss: 0.0025 | Val MAE: $5.88 | Val RMSE: $7.43\n",
            "Epoch 26 | Train Loss: 0.0025 | Val MAE: $4.83 | Val RMSE: $6.20\n",
            "Epoch 27 | Train Loss: 0.0024 | Val MAE: $4.88 | Val RMSE: $6.25\n",
            "Epoch 28 | Train Loss: 0.0024 | Val MAE: $4.73 | Val RMSE: $6.06\n",
            "Epoch 29 | Train Loss: 0.0023 | Val MAE: $4.79 | Val RMSE: $6.10\n",
            "Epoch 30 | Train Loss: 0.0022 | Val MAE: $4.89 | Val RMSE: $6.26\n",
            "Epoch 31 | Train Loss: 0.0021 | Val MAE: $4.72 | Val RMSE: $6.07\n",
            "Epoch 32 | Train Loss: 0.0023 | Val MAE: $4.80 | Val RMSE: $6.41\n",
            "Epoch 33 | Train Loss: 0.0023 | Val MAE: $4.78 | Val RMSE: $6.35\n",
            "Epoch 34 | Train Loss: 0.0022 | Val MAE: $4.73 | Val RMSE: $6.20\n",
            "Epoch 35 | Train Loss: 0.0021 | Val MAE: $4.94 | Val RMSE: $6.35\n",
            "Epoch 36 | Train Loss: 0.0022 | Val MAE: $5.08 | Val RMSE: $6.56\n",
            "Epoch 37 | Train Loss: 0.0021 | Val MAE: $4.75 | Val RMSE: $6.26\n",
            "Epoch 38 | Train Loss: 0.0022 | Val MAE: $5.16 | Val RMSE: $6.66\n",
            "Epoch 39 | Train Loss: 0.0020 | Val MAE: $4.63 | Val RMSE: $5.93\n",
            "Epoch 40 | Train Loss: 0.0019 | Val MAE: $4.66 | Val RMSE: $5.97\n",
            "Epoch 41 | Train Loss: 0.0019 | Val MAE: $4.69 | Val RMSE: $6.03\n",
            "Epoch 42 | Train Loss: 0.0019 | Val MAE: $4.60 | Val RMSE: $6.05\n",
            "Epoch 43 | Train Loss: 0.0020 | Val MAE: $5.29 | Val RMSE: $6.69\n",
            "Epoch 44 | Train Loss: 0.0020 | Val MAE: $4.60 | Val RMSE: $5.94\n",
            "Epoch 45 | Train Loss: 0.0020 | Val MAE: $5.10 | Val RMSE: $6.37\n",
            "Epoch 46 | Train Loss: 0.0020 | Val MAE: $4.60 | Val RMSE: $5.96\n",
            "Epoch 47 | Train Loss: 0.0019 | Val MAE: $4.63 | Val RMSE: $5.93\n",
            "Epoch 48 | Train Loss: 0.0019 | Val MAE: $4.48 | Val RMSE: $5.84\n",
            "Epoch 49 | Train Loss: 0.0020 | Val MAE: $4.74 | Val RMSE: $6.05\n",
            "Epoch 50 | Train Loss: 0.0020 | Val MAE: $4.96 | Val RMSE: $6.32\n",
            "Epoch 51 | Train Loss: 0.0019 | Val MAE: $5.07 | Val RMSE: $6.43\n",
            "Epoch 52 | Train Loss: 0.0019 | Val MAE: $4.51 | Val RMSE: $5.94\n",
            "Epoch 53 | Train Loss: 0.0019 | Val MAE: $4.65 | Val RMSE: $6.00\n",
            "Epoch 54 | Train Loss: 0.0019 | Val MAE: $5.10 | Val RMSE: $6.63\n",
            "Epoch 55 | Train Loss: 0.0020 | Val MAE: $4.50 | Val RMSE: $5.80\n",
            "Epoch 56 | Train Loss: 0.0018 | Val MAE: $4.51 | Val RMSE: $5.85\n",
            "Epoch 57 | Train Loss: 0.0018 | Val MAE: $4.50 | Val RMSE: $5.88\n",
            "Epoch 58 | Train Loss: 0.0021 | Val MAE: $5.57 | Val RMSE: $7.24\n",
            "Epoch 59 | Train Loss: 0.0019 | Val MAE: $4.70 | Val RMSE: $6.28\n",
            "Epoch 60 | Train Loss: 0.0020 | Val MAE: $5.04 | Val RMSE: $6.50\n",
            "Epoch 61 | Train Loss: 0.0019 | Val MAE: $4.66 | Val RMSE: $6.02\n",
            "Epoch 62 | Train Loss: 0.0018 | Val MAE: $5.08 | Val RMSE: $6.63\n",
            "Epoch 63 | Train Loss: 0.0019 | Val MAE: $5.31 | Val RMSE: $6.88\n",
            "Epoch 64 | Train Loss: 0.0020 | Val MAE: $4.46 | Val RMSE: $5.82\n",
            "Epoch 65 | Train Loss: 0.0018 | Val MAE: $4.44 | Val RMSE: $5.78\n",
            "Epoch 66 | Train Loss: 0.0017 | Val MAE: $4.48 | Val RMSE: $5.94\n",
            "Epoch 67 | Train Loss: 0.0019 | Val MAE: $4.51 | Val RMSE: $5.90\n",
            "Epoch 68 | Train Loss: 0.0018 | Val MAE: $4.35 | Val RMSE: $5.75\n",
            "Epoch 69 | Train Loss: 0.0017 | Val MAE: $4.70 | Val RMSE: $6.13\n",
            "Epoch 70 | Train Loss: 0.0017 | Val MAE: $5.55 | Val RMSE: $7.12\n",
            "Epoch 71 | Train Loss: 0.0017 | Val MAE: $4.44 | Val RMSE: $5.75\n",
            "Epoch 72 | Train Loss: 0.0018 | Val MAE: $4.86 | Val RMSE: $6.27\n",
            "Epoch 73 | Train Loss: 0.0016 | Val MAE: $4.39 | Val RMSE: $5.76\n",
            "Epoch 74 | Train Loss: 0.0017 | Val MAE: $5.13 | Val RMSE: $6.49\n",
            "Epoch 75 | Train Loss: 0.0017 | Val MAE: $4.29 | Val RMSE: $5.65\n",
            "Epoch 76 | Train Loss: 0.0017 | Val MAE: $4.95 | Val RMSE: $6.34\n",
            "Epoch 77 | Train Loss: 0.0017 | Val MAE: $4.36 | Val RMSE: $5.89\n",
            "Epoch 78 | Train Loss: 0.0016 | Val MAE: $4.40 | Val RMSE: $5.75\n",
            "Epoch 79 | Train Loss: 0.0016 | Val MAE: $4.40 | Val RMSE: $5.82\n",
            "Epoch 80 | Train Loss: 0.0017 | Val MAE: $5.08 | Val RMSE: $6.47\n",
            "Epoch 81 | Train Loss: 0.0017 | Val MAE: $4.68 | Val RMSE: $6.11\n",
            "Epoch 82 | Train Loss: 0.0016 | Val MAE: $4.46 | Val RMSE: $5.81\n",
            "Epoch 83 | Train Loss: 0.0017 | Val MAE: $4.73 | Val RMSE: $6.10\n",
            "Epoch 84 | Train Loss: 0.0016 | Val MAE: $4.75 | Val RMSE: $6.14\n",
            "Epoch 85 | Train Loss: 0.0017 | Val MAE: $4.69 | Val RMSE: $6.19\n",
            "Epoch 86 | Train Loss: 0.0018 | Val MAE: $4.36 | Val RMSE: $5.79\n",
            "Epoch 87 | Train Loss: 0.0015 | Val MAE: $4.55 | Val RMSE: $5.82\n",
            "Epoch 88 | Train Loss: 0.0016 | Val MAE: $4.65 | Val RMSE: $5.96\n",
            "Epoch 89 | Train Loss: 0.0017 | Val MAE: $4.36 | Val RMSE: $5.70\n",
            "Epoch 90 | Train Loss: 0.0016 | Val MAE: $5.03 | Val RMSE: $6.36\n",
            "Epoch 91 | Train Loss: 0.0020 | Val MAE: $4.53 | Val RMSE: $5.99\n",
            "Epoch 92 | Train Loss: 0.0017 | Val MAE: $4.49 | Val RMSE: $5.86\n",
            "Epoch 93 | Train Loss: 0.0016 | Val MAE: $4.46 | Val RMSE: $5.85\n",
            "Epoch 94 | Train Loss: 0.0015 | Val MAE: $4.54 | Val RMSE: $5.87\n",
            "Epoch 95 | Train Loss: 0.0015 | Val MAE: $4.55 | Val RMSE: $5.97\n",
            "Epoch 96 | Train Loss: 0.0015 | Val MAE: $4.68 | Val RMSE: $6.05\n",
            "Epoch 97 | Train Loss: 0.0015 | Val MAE: $4.34 | Val RMSE: $5.74\n",
            "Epoch 98 | Train Loss: 0.0015 | Val MAE: $4.47 | Val RMSE: $5.74\n",
            "Epoch 99 | Train Loss: 0.0015 | Val MAE: $4.20 | Val RMSE: $5.56\n",
            "Epoch 100 | Train Loss: 0.0015 | Val MAE: $4.42 | Val RMSE: $5.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mamba_forecast(model, dataset, days=32):\n",
        "    \"\"\"Forecast next-day MSFT price using Mamba model\"\"\"\n",
        "    # Ensure model is in eval mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get last valid sequence\n",
        "    last_idx = len(dataset) - 1\n",
        "    if last_idx < days:\n",
        "        raise ValueError(f\"Need at least {days} days of historical data\")\n",
        "\n",
        "    # Prepare input tensor\n",
        "    x = dataset.X[:, last_idx-days:last_idx].T  # [seq_len, num_features]\n",
        "    x = torch.tensor(x, dtype=torch.float32, device=device)\n",
        "    x = x.unsqueeze(0).to(device)  # Add batch dim and move to device\n",
        "\n",
        "    # Make prediction\n",
        "    with torch.no_grad():\n",
        "        pred_normalized = model(x)\n",
        "\n",
        "    # Convert to actual price\n",
        "    pred_price = dataset.inverse_transform(pred_normalized.cpu().item())\n",
        "\n",
        "    # Get reference price\n",
        "    last_price = dataset.inverse_transform(dataset.Y[last_idx])\n",
        "\n",
        "    return pred_price, last_price\n",
        "\n",
        "# Modified evaluate_model with device handling\n",
        "def evaluate_model(dataloader, model, dataset):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    predictions = []\n",
        "    truths = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)  # Move to model's device\n",
        "            preds = model(X)\n",
        "\n",
        "            # Move back to CPU for numpy conversion\n",
        "            preds_denorm = dataset.inverse_transform(preds.cpu().numpy().flatten())\n",
        "            y_denorm = dataset.inverse_transform(y.cpu().numpy().flatten())\n",
        "\n",
        "            predictions.extend(preds_denorm)\n",
        "            truths.extend(y_denorm)\n",
        "\n",
        "    mae = np.mean(np.abs(np.array(predictions) - np.array(truths)))\n",
        "    rmse = np.sqrt(np.mean((np.array(predictions) - np.array(truths))**2))\n",
        "    return mae, rmse\n",
        "\n",
        "# Get prediction\n",
        "try:\n",
        "    pred_price, last_price = mamba_forecast(mamba_model, stockData, days=5)\n",
        "\n",
        "    print(\"\\n📈 MSFT Stock Forecast using Mamba Network\")\n",
        "    print(f\"│ Last Closing Price:   ${last_price:.2f}\")\n",
        "    print(f\"│ Predicted Next Close: ${pred_price:.2f}\")\n",
        "    print(f\"│ Expected Change:      +{pred_price - last_price:.2f} (+{(pred_price/last_price-1)*100:.2f}%)\")\n",
        "\n",
        "    # Show model accuracy\n",
        "    val_mae, val_rmse = evaluate_model(valid_dataloader, mamba_model, stockData)\n",
        "    print(f\"\\n📊 Model Validation Metrics\")\n",
        "    print(f\"│ Typical Error (MAE):  ±${val_mae:.2f}\")\n",
        "    print(f\"│ Max Likely Error:     ${val_rmse:.2f} (RMSE)\")\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Forecast error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AWITI-6r4rA",
        "outputId": "f7a204a8-cb30-4c46-bbed-03ffee940094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 MSFT Stock Forecast using Mamba Network\n",
            "│ Last Closing Price:   $378.77\n",
            "│ Predicted Next Close: $389.99\n",
            "│ Expected Change:      +11.22 (+2.96%)\n",
            "\n",
            "📊 Model Validation Metrics\n",
            "│ Typical Error (MAE):  ±$4.42\n",
            "│ Max Likely Error:     $5.75 (RMSE)\n"
          ]
        }
      ]
    }
  ]
}